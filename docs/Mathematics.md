---
layout: default
title: Mathematics
nav_order: 4
---

# Mathematics
- 2021.03.24 Updated
- source from: (https://brunch.co.kr/@chris-song/69).

---
## 1. Different Distances

### 1) Kullback-Leibler Divergence
- KL Divergence는 두 확률 분포를 비교하기 위해 사용된다. KL Divergence의 공식은 아래와 같다.  
- Bayesian Inference에서의 KL-Divergence의 의미로 __q(bold)__는 사전확률분포, p는 사후확률분포이다. 이때, KL(p∥q)는 사전확률에서 사후확률로 변할때 얻은 정보의 양으로 해석할 수 있다.  
- p와 q의 일반적인 의미로 p는 참 분포(True Distribution) 즉, 실제 관찰 데이터(Observation)을 의미한다. q는 주로 가설(Theory), 모델(Model), p의 근사(Approximation)로 사용된다.

<img src='/figure/formula/KLD.PNG' height="60%" width="60%" align="center"/>  
  
### 2) Jensen-Shanon Divergence
a
### 3) Total Variation

### 4) Earth Mover Distance
